---
title: "Data Manipulation in R"
author: "Dr Hannah Buckland"
#date: "10/01/2022"
output:
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

One of my favourite aspects of using R is how easy it is to manipulate data. Here I will go through a short basic example of you we can use R to read in and tidy up some probe data from two different analysis sessions that are saved as Excel worksheets. I will then show how easy it is to then combine this data with another file containing information gathered from the field.

## Manipulating EPMA data
### Reading in the data

First we need to read in our data sets:
```{r packages and data read, warning=FALSE, message=FALSE}
library(readxl) # an R package that allows you to easily work with Excel workbooks
library(tidyverse) # set of packages that I use for data manipulation and plotting
library(stringr) # package for dealing with strings 
library(data.table) # another powerful R package for data manipulation
library(here)

probe_files <- list.files(path = "../data", pattern = "probe", full.names = TRUE) # Look in the data folder and list the files with "probe" in the file name
file_names <- str_extract(probe_files, pattern = "(?<=/data/).*(?=.xlsx)") # extracts the string after "/" and before ".xlsx" 

# Read in each workbook as a list element
probe_all <- lapply(probe_files, FUN = read_excel)
names(probe_all) <- file_names # assigns names to each list element from file_names vector

```
### Combining two Excel spreadsheets containing geochemical data

We now have read in both probe datasets. These files are in the Excel format that is directly output from the JEOL microprobe at the University of Bristol using the Probe for EPMA software. 

*However*, even though collected using the same probe and software, there are some differences in the Excel files. I must have chosen to output the files slightly differently following each session, which is something that can easily happen.

Lets have a look at the column headers of each file:

```{r print headers of each file}

# Extract the column names for each dataframe within the list
names_1 <- names(probe_all$probe_data_day1) 
names_2 <- names(probe_all$probe_data_day2)

print(names_1)
length(names_1) # this prints the number of columns in dataset 1
print(names_2)
length(names_2) # this prints the number of columns in dataset 1
```

We see by interrogating column names of these files that we have mismatches. If we simply processed this data in Excel (which was the approach I used in the past) this would be very irritating. For instance you would have to move around the order of your oxide columns. However, in R you can avoid this annoying step and automatically match up columns that have the same header:

```{r comparing columns}
# Find the columns the exist in both datasets
diff_names <- intersect(names_1,names_2)
print(diff_names)

# Bind the two dataframes together into one large dataframe using data.table
all_probe_data <- data.table::rbindlist(probe_all, fill = TRUE, idcol = TRUE) # by setting fill = TRUE this fills the columns that don't have data and using idcol = TRUE we add a variable for the analysis day called ".id"

# Now we can reduce this dataset to only include the columns where we have data for both analysis sessions
all_probe_filter <- subset(all_probe_data, select = diff_names) 

# However we have now lost the ".id" column as this was not in the original data frames so let's add that back in

all_probe_filter <- all_probe_filter %>%
  mutate(id = all_probe_data$.id)

```

Now we have a long format data frame containing the data from both analysis days. You will see that we don't have any empty columns either and we didn't have to reorder the columns manually. Please note, in general the "intersect" and "rbindlist" functions are really powerful when comparing and manipulation lists and data frames in R.

### Post-processing and tidying up EPMA data 

You will see that the EPMA data contains a mixture of unknowns (samples with the prefix MZ for Mazama tephra) as well as secondary standard analyses (a mixture of Lipari and KN18 glasses). 

We can look at the secondary standard data later, first let's focus on the Mazama unknowns:
```{r filtering for unknowns}

# Set up new data frame for rows where the sample name contains 'MZ'
unknowns <- all_probe_filter %>%
  filter(grepl('MZ', SAMPLE))

```

Now we have all the unknowns in one place, we might want to process the data. Firstly, we can normalise the oxides and find the volatiles by difference. Again, these steps can be done in Excel, but this method leaves you open to copying/dragging across the wrong formula especially when dealing with big datasets. Alternatively, in R, row-wise operations are easily carried out and easily repeated when processing new files and datasets.

Let's normalise the oxide data:
```{r finding the volatiles by difference}

# First we need to isolate the oxide columns
unk_cols <- data.frame(names=colnames(unknowns))

# This will get the column names excluding the detection limit, error and date-time columns
unk_cols_ox <- unk_cols %>% 
  filter(!grepl('CDL|ERR|DATETIME',names)) %>%
  unlist(use.names = FALSE) # this changes the output of the filter to a vector

# Now we subset the unknowns dataframe to only include the oxide, total and sample columns and normalise the data to 100%
unknowns_norm <- unknowns %>%
  select(all_of(unk_cols_ox)) %>%
  mutate(across(.cols = CaO:O,
                .fns = ~ .x / TOTAL *100)) %>% # this across function only performs the function on the specified columns 
  mutate(across(.cols = CaO:TOTAL,
                .fns = ~ round(.x,3))) %>%
  mutate(H2O_diff = 100 - TOTAL) # here we also add a column of volatiles by difference

# Print the first 6 rows to check that has worked
head(unknowns_norm)

```

We might at this stage also want to exclude some data with anomalous totals or use other limits on certain oxides to remove any "dud" analyses. I won't go into this in detail here because it can be sample specific, but this code block shows some ways you could approach the filtering stage. Also please see the writing functions markdown document for how to write and call functions:
```{r data clean function}

# I want to use the "data_clear_function" I have saved separately
source(here::here("code","data_clean_function.R"))

# This runs that function on our normalised data
unknowns_clean <- data_clean(unknowns_norm)

removed <- nrow(unknowns_norm)-nrow(unknowns_clean) # compare the number of rows

print(paste("This function has excluded",removed,"analyses"))

```

## Combining EPMA and field data

The data frame "unknowns_clean" is now ready to be interrogated for source volcano correlations or other analysis methods. It is also in a format that it can be easily matched up with files that contain parallel information. For this example I will show how we can match up the EMPA data with an Excel spreadsheet I use to keep track of sample prep and a csv file containing information about the sampling locations.

### Reading in the files containing the information about our field sites
```{r reading in field and sample data}

# Read in the Excel spreadsheet
sample_prep <- read_excel("../data/MZ_Sample_Prep.xlsx")

# Read in the csv file containing the lat long of each locality in the Mazama Locality Database (Buckland et al. 2020)

MLDB <- read.csv("../data/MLDB_latlong.csv",header=TRUE)
MLDB <- MLDB %>%
  mutate(MLDB_Number = as.character(MLDB$MLDB_Number)) # we change this column to a character to avoid R thinking it is an integer or factor
```

Now we can see there are some common columns across all of these data sets and I would encourage everyone to design a good sample numbering/naming system that can be used from the field data collection stage right through to EPMA and publication. For instance, every sample I analyse in the lab be it for grain size, componentry or geochemistry, it gets given a unique "Lab_ID":
```{r print Lab_ID}
head(sample_prep$Lab_ID)
```
And you can see I use these when naming the individual probe analyses with each getting an analytical ID, which gets termed "SAMPLE" by the Probe for EPMA software:

```{r print analytical id}
head(unknowns_clean$SAMPLE)
```
This means we can quite easily merge the two datasets.

The same is then true when working between the "Sample_Prep" Excel workbook and the csv file containing the lat long information for each Mazama locality. Here it is the MLDB_numb columns that match up:

```{r print MLDB info}
head(sample_prep$Locality_MLDB)
head(MLDB$MLDB_Number)
```

Now, with some slight manipulation, we can add the information contained in the sample prep and MLDB files to our geochemistry dataset:

```{r merging datasets}

# First we need to extract the Lab_ID from the SAMPLE column in the geochemistry data
unknowns_mod <- unknowns_clean %>%
  mutate(Lab_ID = sub("\\_.*", "", SAMPLE)) # This extracts the string before the "_"

#Now we can add the locality number from the sample prep data frame
sample_prep_sub <- sample_prep %>%
  select(Locality_MLDB,Lab_ID,Fines_GSD,Notes) # this only selects a few columns we want to combine, but you can easily include additional columns

unknowns_mod <- merge(unknowns_mod,
                      sample_prep_sub,
                      by="Lab_ID") # this tells R which columns to match up

# Now we want to add in lat long data from the csv file
unknowns_mod <- merge(x=unknowns_mod,
                      y=MLDB,
                      by.x="Locality_MLDB",
                      by.y="MLDB_Number") # this is an example of how you can merge data frames even when the column headers are different using x and y

```

Following these steps we now have a data frame that contains all of the EPMA data as well as a the field information (sample description and lat-long of where the sample was collected) and information about whether the sample has also undergone grain size analysis.


### Why is this useful?

Being able to easily match up different type of information (in this example EPMA data, lab analysis and field data) is very useful for physical volcanology studies where we are carrying out multiple analyses on the same sample. It is also very useful if you have multiple authors working on the same sample sets as the worksheets can easily be shared on GitHub so there is a centralised record of what has happened to the sample at every stage of analysis.

It also allows for some nice data visualisations to be generated. For example, we might want to visualise the spatial distribution of our EPMA data:

```{r spatial distribution of field data, fig.align='center', echo=FALSE, message=FALSE, warning=FALSE}

# This code chunk contains some ways we can create maps in R please see the R Markdown document for making maps with R for more details
library(sf) # package for spatial data
library(rnaturalearth) # package that includes map data

# Read in data for plotting map outlines of countries
statesdata <- ne_states(country =c("United States of America","Canada"),
                            returnclass = 'sf')

volcano <- data.frame(name="Mazama",
                        latitude = 42.9446,
                        longitude = -122.1090) # data frame with location of mount Mazama

volc_sf <- st_as_sf(volcano,
                    coords = c("longitude","latitude"),crs = 4326, 
                    agr = "constant") # convert to spatial dataset

# Calculate number of analyses per site
localities_geochem <- unknowns_mod %>%
  count(Locality_MLDB)
# Add back in the lat long information for each site
localities_geochem <- merge(x=localities_geochem,
                      y=MLDB,
                      by.x="Locality_MLDB",
                      by.y="MLDB_Number")

# Now convert this to a spatial data set for plotting
loc_sf <- st_as_sf(localities_geochem, 
                   coords = c("Longitude", "Latitude"), crs = 4326, 
                   agr = "constant") # convert to spatial dataset

ggplot(data=statesdata) +
   geom_sf(fill = "#F4F9E9",colour = "#2F323A",size=0.2) +
  geom_sf(data=loc_sf,
          aes(colour=Locality_MLDB,
              size=n)) +
  geom_sf_text(data = loc_sf,
               aes(label=Locality_MLDB),
               nudge_x = 0.25,
               nudge_y = 0.25,
               size=3) +
  geom_sf(data=volc_sf,
          pch=17,
          size=4,
          colour = "black") +
  scale_size_binned(breaks= c(5,10,25,50,100)) +
  labs(color="MLDB Number", size = "Number of Analyses") +
  xlab(NULL) +
  ylab(NULL) +
  ggtitle("Visualising the spatial distribution of Mazama glass analyses") + 
  coord_sf(xlim=c(-125,-115),ylim=c(40.5,50)) +
  theme(panel.background = element_rect(fill="#B4D6D3"),
        legend.position = "right")


```

From this plot for instance we can see that our analyses are very biased towards one locality (MLDB number 46).

## Take-home messages

* R can replace or supplement Excel for manipulating data sets in volcanology and tephra studies (e.g., EPMA data, field and laboratory data)
* The [tidyverse](https://www.tidyverse.org/) and [data.table](https://cran.r-project.org/web/packages/data.table/vignettes/datatable-intro.html) packages are powerful for data manipulation and processing
* Designing comprehensive sample numbering systems will facilitate easy work flows across multiple methodologies
 
<hr />


